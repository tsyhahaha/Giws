defaults:
 - model: lstm

target: lstm

# general
gpu_id: 0
seed: 42
verbose: false

# training
data_path: /mnt/user/taosiyuan/projects/data/simple-examples/data
seq_len: 64
epochs: 100
batch_size: 32
lr: 3e-2

# ref: https://github.com/tmatha/lstm/blob/master/lstm_tf.ipynb
# batch_size=20
# seq_len=20
# clip_norm=5
# learning_rate=1.
# decay=0.5
# epochs=13
# epochs_no_decay=4

# optim
schedule_type: 'cosine'
optim:
  total_steps: 3000
  min_lr: 1e-2

# gradient clip
clip_grad: true
clip_mode: "norm"
clip_grad_value: 1.0

# amp
amp_enabled: false

# eval and save
eval: true
eval_interval: 1
save_interval: 100
output_dir: ./trainer_output/lstm